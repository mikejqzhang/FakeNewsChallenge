\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
% \usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{The Fake News Challenge as Natural Language Inference}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Author \\
  Department \\
  University \\
  Address \\
  \texttt{email} \\
  %% examples of more authors
  \And
  Author \\
  Department \\
  University \\
  Address \\
  \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  The Fake News Challenge defines a task and dataset for stance detection where the goal is to
  determine the semantic relationship between an article's headline and body as either
  agrees with, disagrees with, discusses, or is unrelated to the headline. This task is very similar
  to the task of natural language inference where popular datasets are of the form given a premise
  and hypothesis sentence, the goal is to predict whether the premis entails, contradicts,
  or is neither (neutral) to the hypothesis sentence. Despite the similarities between these two
  tasks, the none top performing 3 systems from the Fake News Challenge utilize state of
  the art systems in natural language inference. In this work, we attempt to approach the
  Fake News Challenge as a natural language inference task, and try to understand if top performing
  natural language inference models perform similarly well (or even better)
  on this related task. We additionally use our results and ablations to try and understand what are
  the core similarities and differences between these two task.
\end{abstract}

\section{Introduction}

One commonality between the three top performing models on the Fake News Challenge is that they
all utilize some deep neural model to get some representation of both the headline and the body,
then use some multi-layer perceptron to combine the representations together to form
a prediction. This heavily resembles the pattern seen in older works in natural language inference
from around 2015-2016. This isn't surprising considering how related these two tasks seem to be,
both requiring understanding and clasifying the semantic relationship between two bodies of text.

As some background, natural language inference is a widely researched task in natural language
processing and is core to the overall goal of natural language understanding.
Popular natural language inference datasets [][] formulate the task is given a pair a sentences
$(p, h)$ where $p$ is the premise and $h$ is the hypotheses, we must determine whether $p$
semantically entails $h$, contradicts $h$ or is neutral to $h$.

Models for natural language inference were previously also dominated by deep models (often times
some deep convolutional or recurrent nerual network) that encode both the premise and hypothesis
sentences into low dimensional latent representations which intend to capture the semantic
content of the sentence. These models then combine two representations via some multi-layer
perceptron or classification model. Since then models have been able to get performance gains
by utilizing attention as a way to combine information across texts and force the model to
produce representations of each text that are dependent on the content of their counterpart.
We hypothesize that using similar techniques for the Fake News Challenge dataset will also
yield similar gains as in the natural language inference setting.

\subsection{Hand Crafted Features}
Another commonality between each of the top performing models on the Fake News Challenge
is that they all use hand crafted features (such as TF-IDF, sentiment, etc)
to supplement the deep model. Another goal we have is to understand the efficacy of these hand
crafted features and how vital they are for the performance of the system. We plan to do this
by incorperating them into our deep model and doing ablation testing.

\subsection{Dataset Artifact Analysis}
Something that was a recent finding in natrual language inference datsets is that there
were annotation artifacts in the data that meant models could with considerably high accuracy
predict the entail/contradict/neutral label by only observing the hypothesis sentence, completely
ignoring the premise. Although the source of these annotation artifacts in other datasets
seems to be a result of crowdsourcing data, which the Fake News Challenge does not do, it
still would be useful to do the same type of testing that was used in Suchin et al. to understand
the dataset further.

\subsection{Contextual Embeddings}
Another extension we plan on attempting (if time permits) is using contextual embeddings to
replace the standard pretrained word vectors for out models. The current state of the art in
natural language inference (as well as many other natural language processing tasks) is achieved
through applying these contextual embedding systems, and we hope to see similar gains by adding
these to our model.


\section{Dataset Artifact Analysis}

\section{Results}

\subsection{Figures}

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
\end{figure}



\section{Citations, figures, tables, references}
\label{others}


All artwork must be neat, clean, and legible. Lines should be dark
enough for purposes of reproduction. The figure number and caption
always appear after the figure. Place one line space before the figure
caption and one line space after the figure. The figure caption should
be lower case (except for first word and proper nouns); figures are
numbered consecutively.

You may use color figures.  However, it is best for the figure
captions and the paper body to be legible if the paper is printed in
either black/white or in color.

\subsection{Tables}

All tables must be centered, neat, clean and legible.  The table
number and title always appear before the table.  See
Table~\ref{sample-table}.

Place one line space before the table title, one line space after the
table title, and one line space after the table. The table title must
be lower case (except for first word and proper nouns); tables are
numbered consecutively.

Note that publication-quality tables \emph{do not contain vertical
  rules.} We strongly suggest the use of the \verb+booktabs+ package,
which allows for typesetting high-quality, professional tables:
\begin{center}
  \url{https://www.ctan.org/pkg/booktabs}
\end{center}
This package was used to typeset Table~\ref{sample-table}.

\begin{table}
  \caption{Sample table title}
  \label{sample-table}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
\end{table}

\subsubsection*{Acknowledgments}

Use unnumbered third level headings for the acknowledgments. All
acknowledgments go at the end of the paper. Do not include
acknowledgments in the anonymized submission, only in the final paper.

\newpage
\section*{References}
\small

[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms
for connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and
T.K.\ Leen (eds.), {\it Advances in Neural Information Processing
  Systems 7}, pp.\ 609--616. Cambridge, MA: MIT Press.

[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS:
  Exploring Realistic Neural Models with the GEneral NEural SImulation
  System.}  New York: TELOS/Springer--Verlag.

[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of
learning and recall at excitatory recurrent synapses and cholinergic
modulation in rat hippocampal region CA3. {\it Journal of
  Neuroscience} {\bf 15}(7):5249-5262.

\end{document}
